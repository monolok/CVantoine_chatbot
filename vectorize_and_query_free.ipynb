{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from transformers import Pipeline\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pdf_files = Path(\"data\").glob(\"*.pdf\")\n",
    "text = \"\"\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    reader = PdfReader(pdf_file)\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=40,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "# for doc in text:\n",
    "#     docs_processed += text_splitter.split_documents([doc])\n",
    "#docs_processed += text_splitter.split_documents(text)\n",
    "docs_processed += text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embed and save the VDB for free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!KERNEL CRASH\n",
    "# EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=EMBEDDING_MODEL_NAME,\n",
    "#     multi_process=True,\n",
    "#     model_kwargs={\"device\": \"cpu\"}, #cuda\n",
    "#     encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.038338541984558105, 0.1234646886587143, -0.028642907738685608]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kernel test\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_texts(\n",
    "    docs_processed, embeddings, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick querry test\n",
    "query_text = \"Does Antoine speak spanish?\"\n",
    "KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=query_text, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the VDB with langchain instance\n",
    "KNOWLEDGE_VECTOR_DATABASE.save_local(\"open_source_model_vdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#! Friendly reminder -> cell to run if cell above not ran \n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vdb\n",
    "vdb = FAISS.load_local(\"open_source_model_vdb\", embeddings, allow_dangerous_deserialization=True) #  take the same model used for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "# quick querry test\n",
    "query_text = \"Does Antoine speak spanish?\"\n",
    "docs = vdb.similarity_search(query=query_text, k=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Antoine Bertin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add LLM Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context, give a comprehensive answer to the question. \n",
    "        Respond only to the question asked, response should be concise and relevant to the question. \n",
    "        Do not respond anything that can't be answered from the context. If the answer cannot be deduced from the context suggest the following:\n",
    "- Schedule a video call with Antoine Bertin for more detailed information. You can do this by visiting his Calendly link: 'https://calendly.com/antoinebertin/30'.\n",
    "- Learn more about Antoine's story, you can download the 'antoine' package. To do this, use the command 'pip install antoine' in your terminal and run the 'hire_antoine' command.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [01:24<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#)\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME)#, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prompt template\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "#print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = RAG_PROMPT_TEMPLATE.format(question=\"who is antoine?\", context=\"antoine is a famous actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = READER_LLM(final_prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antoine is referred to as a \"famous actor\" in the given context. Therefore, Antoine is likely a well-known individual in the entertainment industry who works primarily as an actor. Without further context, it is impossible to provide additional details about this person\\'s identity or career. If you would like more information about Antoine, I suggest scheduling a video call with him through his Calendly link (https://calendly.com/antoinebertin/30) or learning more about his story by downloading the \"antoine\" package using the commands \"pip install antoine\" and \"run hire_antoine\" in your terminal.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer # took 2m20sec in codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "# ]\n",
    "#answer = READER_LLM(messages)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas de reranker sur un petit doc\n",
    "# from ragatouille import RAGPretrainedModel\n",
    "# RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "# TODO https://huggingface.co/learn/cookbook/en/advanced_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    num_retrieved_docs: int = 2,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Generating answer...\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Does Antoine speak spanish?\"\n",
    "\n",
    "answer, relevant_docs = answer_with_rag(query_text, READER_LLM, vdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, according to the context provided, Antoine speaks Spanish as one of his languages.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Yes, according to the context provided, Antoine speaks Spanish as one of his languages.'\n",
    "\n",
    "def stream_response(response):\n",
    "    \"\"\"Yields responses from the AI, replacing placeholders as needed.\"\"\"\n",
    "    try:\n",
    "        for r in response:\n",
    "            #content = r.choices[0].delta.content\n",
    "            # prevent $ from rendering as LaTeX\n",
    "            #content = content.replace(\"$\", \"\\$\")\n",
    "            #yield content\n",
    "            yield r\n",
    "    except Exception as e:\n",
    "        yield f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def stream_str(s, speed=250):\n",
    "    \"\"\"Yields characters from a string with a delay to simulate streaming.\"\"\"\n",
    "    print(s)\n",
    "    for c in s:\n",
    "        return c\n",
    "        time.sleep(1 / speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "o\n",
      "r\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m stream_response(test):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(c)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for c in stream_response(test):\n",
    "    print(c)\n",
    "    time.sleep(1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object stream_response at 0x77e590480820>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_str(stream_response(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:',\n",
       " \"If you want to contribute to the 'antoine' package and make me better, check out my GitHub: [http://github.com/monolok/antoine](http://github.com/monolok/antoine)\\nSkills:\\n* Project Management\\n* Business Development\\n* Process Improvement\\n* Process Automation\\n* Cluster Analysis\\n* System Deployment\\n* Prediction\\n* Deep Learning\\n* Machine Learning\\n* Big Data\\n* Big Data Analytics\\n* Data Mining\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
