{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "#from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = Path(\"data\").glob(\"*.pdf\")\n",
    "text = \"\"\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    reader = PdfReader(pdf_file)\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=40,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "# for doc in text:\n",
    "#     docs_processed += text_splitter.split_documents([doc])\n",
    "#docs_processed += text_splitter.split_documents(text)\n",
    "docs_processed += text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embed and store to VDB for free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
    "#print(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store chunks with unique keys\n",
    "chunk_dict = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "# Save chunks to a JSON file\n",
    "with open(\"data/cv_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_dict, f)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "def embed(input: str):\n",
    "    return client.embeddings(\"mistral-embed\", input=input).data[0].embedding\n",
    "\n",
    "\n",
    "embeddings = np.array([embed(chunk) for chunk in chunks])\n",
    "dimension = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"data/vector_cv.index\")\n",
    "print(f\"Index saved with {len(embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read VDB, query w/ COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 73 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved FAISS index\n",
    "index = faiss.read_index(\"data/vector_cv.index\")\n",
    "\n",
    "# Load the text chunks from JSON file\n",
    "with open(\"data/cv_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_dict = json.load(f)\n",
    "\n",
    "# Check if the chunks are loaded correctly\n",
    "print(f\"Loaded {len(chunk_dict)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query test w/ MISTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "client = MistralClient(api_key=api_key)\n",
    "def embed(input: str):\n",
    "    return client.embeddings(\"mistral-embed\", input=input).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MistralAPIException",
     "evalue": "Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMistralAPIException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example query embedding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes Antoine speak spanish?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embed(query_text)])\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36membed\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39membeddings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:283\u001b[0m, in \u001b[0;36mMistralClient.embeddings\u001b[0;34m(self, model, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m request \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m}\n\u001b[1;32m    281\u001b[0m singleton_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m singleton_response:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmbeddingResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MistralException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response received\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:142\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt, data, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    134\u001b[0m             method,\n\u001b[1;32m    135\u001b[0m             url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    140\u001b[0m         )\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralConnectionException(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:75\u001b[0m, in \u001b[0;36mMistralClient._check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response_status_codes(response)\n\u001b[1;32m     77\u001b[0m     json_response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:60\u001b[0m, in \u001b[0;36mMistralClient._check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m     59\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[1;32m     61\u001b[0m         response,\n\u001b[1;32m     62\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream:\n",
      "\u001b[0;31mMistralAPIException\u001b[0m: Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}"
     ]
    }
   ],
   "source": [
    "# Example query embedding\n",
    "query_text = \"Does Antoine speak spanish?\"\n",
    "query_embedding = np.array([embed(query_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Search the index\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m D, I \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_embedding, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Top-2 nearest chunks with D -> Distance and I -> Index\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Retrieve the most relevant chunks\u001b[39;00m\n\u001b[1;32m      5\u001b[0m retrieved_chunks \u001b[38;5;241m=\u001b[39m [chunk_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m I[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Search the index\n",
    "D, I = index.search(query_embedding, k=2)  # Top-2 nearest chunks with D -> Distance and I -> Index\n",
    "\n",
    "# Retrieve the most relevant chunks\n",
    "retrieved_chunks = [chunk_dict[f\"{i}\"] for i in I[0]]\n",
    "\n",
    "print(\"Retrieved chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query function w/ MISTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ask(query: str, index, chunk_dict):\n",
    "    embedding = embed(query)\n",
    "    embedding = np.array([embedding])\n",
    "\n",
    "    _, indexes = index.search(embedding, k=2)\n",
    "    context = [chunk_dict[f\"{i}\"] for i in indexes.tolist()[0]]\n",
    "\n",
    "    user_message = prompt.format(context=context, query=query)\n",
    "\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "    chat_response = client.chat(model=\"mistral-medium\", messages=messages)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MistralAPIException",
     "evalue": "Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMistralAPIException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ask(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat work experience does he have?\u001b[39m\u001b[38;5;124m\"\u001b[39m, index, chunk_dict)\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mask\u001b[0;34m(query, index, chunk_dict)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, index, chunk_dict):\n\u001b[0;32m---> 13\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embed(query)\n\u001b[1;32m     14\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embedding])\n\u001b[1;32m     16\u001b[0m     _, indexes \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(embedding, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36membed\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39membeddings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:283\u001b[0m, in \u001b[0;36mMistralClient.embeddings\u001b[0;34m(self, model, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m request \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m}\n\u001b[1;32m    281\u001b[0m singleton_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m singleton_response:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EmbeddingResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MistralException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response received\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:142\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt, data, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    134\u001b[0m             method,\n\u001b[1;32m    135\u001b[0m             url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    140\u001b[0m         )\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralConnectionException(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:75\u001b[0m, in \u001b[0;36mMistralClient._check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response_status_codes(response)\n\u001b[1;32m     77\u001b[0m     json_response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mistralai/client.py:60\u001b[0m, in \u001b[0;36mMistralClient._check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m     59\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[1;32m     61\u001b[0m         response,\n\u001b[1;32m     62\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream:\n",
      "\u001b[0;31mMistralAPIException\u001b[0m: Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}"
     ]
    }
   ],
   "source": [
    "ask(\"What work experience does he have?\", index, chunk_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
