{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from transformers import Pipeline\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pdf_files = Path(\"data\").glob(\"*.pdf\")\n",
    "text = \"\"\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    reader = PdfReader(pdf_file)\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=40,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "# for doc in text:\n",
    "#     docs_processed += text_splitter.split_documents([doc])\n",
    "#docs_processed += text_splitter.split_documents(text)\n",
    "docs_processed += text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embed and save the VDB for free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, list)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed), type(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!KERNEL CRASH\n",
    "# EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=EMBEDDING_MODEL_NAME,\n",
    "#     multi_process=True,\n",
    "#     model_kwargs={\"device\": \"cpu\"}, #cuda\n",
    "#     encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.038338541984558105, 0.1234646886587143, -0.028642907738685608]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kernel test\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_texts(\n",
    "    docs_processed, embeddings, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick querry test\n",
    "query_text = \"Does Antoine speak spanish?\"\n",
    "KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=query_text, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the VDB with langchain instance\n",
    "KNOWLEDGE_VECTOR_DATABASE.save_local(\"open_source_model_vdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#! Friendly reminder -> cell to run if cell above not ran \n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vdb\n",
    "vdb = FAISS.load_local(\"open_source_model_vdb\", embeddings, allow_dangerous_deserialization=True) #  take the same model used for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "# quick querry test\n",
    "query_text = \"Does Antoine speak spanish?\"\n",
    "docs = vdb.similarity_search(query=query_text, k=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Antoine Bertin'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add LLM Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context, give a comprehensive answer to the question. \n",
    "        Respond only to the question asked, response should be concise and relevant to the question. \n",
    "        Do not respond anything that can't be answered from the context. If the answer cannot be deduced from the contextcsuggest the following:\n",
    "- Schedule a video call with Antoine Bertin for more detailed information. You can do this by visiting his Calendly link: 'https://calendly.com/antoinebertin/30'.\n",
    "- Learn more about Antoine's story, you can download the 'antoine' package. To do this, use the command 'pip install antoine' in your terminal and run the 'hire_antoine' command.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [01:22<00:00, 10.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#)\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME)#, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Using the information contained in the context, give a comprehensive answer to the question. \n",
      "        Respond only to the question asked, response should be concise and relevant to the question. \n",
      "        Do not respond anything that can't be answered from the context. If the answer cannot be deduced from the contextcsuggest the following:\n",
      "- Schedule a video call with Antoine Bertin for more detailed information. You can do this by visiting his Calendly link: 'https://calendly.com/antoinebertin/30'.\n",
      "- Learn more about Antoine's story, you can download the 'antoine' package. To do this, use the command 'pip install antoine' in your terminal and run the 'hire_antoine' command.</s>\n",
      "<|user|>\n",
      "Context:\n",
      "{context}\n",
      "---\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: {question}</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check prompt template\n",
    "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' 8\\n\\nQuestion: How many sides does a triangle have? Answer: 3\\n\\nQuestion: Which planet is known as the Red Planet? Answer: Mars\\n\\nQuestion: Who painted the Mona Lisa? Answer: Leonardo da Vinci\\n\\nQuestion: Which country won the FIFA World Cup in 2018? Answer: France\\n\\nQuestion: Who wrote the novel \"Pride and Prejudice\"? Answer: Jane Austen\\n\\nQuestion: Which animal is found on the Australian coat of arms? Answer: Kangaroo\\n\\nQuestion: In which ocean is the Great Barrier Reef located? Answer: Coral Sea (which is part of the Pacific Ocean)\\n\\nQuestion: Which country hosted the 2016 Summer Olympics? Answer: Brazil\\n\\nQuestion: Which famous landmark is located in New York City? Answer: Statue of Liberty\\n\\nQuestion: Which continent is the largest in terms of area? Answer: Asia\\n\\nQuestion: Which country is known for its famous cherry blossoms? Answer: Japan\\n\\nQuestion: Which country has the longest coastline in the world? Answer: Canada\\n\\nQuestion: Which country is home to the highest waterfall in the world? Answer: Venezuela\\n\\nQuestion: Which country is known for its famous tulips? Answer: Netherlands\\n\\nQuestion: Which country is known for its famous fjords? Answer: Norway\\n\\nQuestion: Which country is known for its famous wine regions? Answer: France\\n\\nQuestion: Which country is known for its famous ski resorts? Answer: Switzerland\\n\\nQuestion: Which country is known for its famous opera houses? Answer: Italy\\n\\nQuestion: Which country is known for its famous canals? Answer: Netherlands\\n\\nQuestion: Which country is known for its famous beaches? Answer: Australia\\n\\nQuestion: Which country is known for its famous deserts? Answer: United Arab Emirates\\n\\nQuestion: Which country is known for its famous rainforests? Answer: Brazil\\n\\nQuestion: Which country is known for its famous mountains? Answer: Nepal\\n\\nQuestion: Which country is known for its famous lakes? Answer: Switzerland\\n\\nQuestion: Which country is known for its famous volcanoes? Answer: Indonesia\\n\\nQuestion: Which country is known for its famous glaciers? Answer: Iceland\\n\\nQuestion: Which'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "READER_LLM(\"What is 4+4? Answer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pas de reranker sur un petit doc\n",
    "# from ragatouille import RAGPretrainedModel\n",
    "# RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "# TODO https://huggingface.co/learn/cookbook/en/advanced_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    num_retrieved_docs: int = 2,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Generating answer...\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Does Antoine speak spanish?\"\n",
    "\n",
    "answer, relevant_docs = answer_with_rag(query_text, READER_LLM, vdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, according to the context provided, Antoine speaks Spanish as one of his languages.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Antoine Bertin\\nAge: 35 years old\\nOccupation: Data Scientist, Former Sommelier & Blockchain Enthusiast\\nLink to schedule a call: [https://calendly.com/antoinebertin/30](https://calendly.com/antoinebertin/30)\\nEmail: [antoinebe35gmail.com](mailto:antoinebe35gmail.com)\\nLanguages:\\n* French\\n* Spanish\\n* English\\nAbout me:',\n",
       " \"If you want to contribute to the 'antoine' package and make me better, check out my GitHub: [http://github.com/monolok/antoine](http://github.com/monolok/antoine)\\nSkills:\\n* Project Management\\n* Business Development\\n* Process Improvement\\n* Process Automation\\n* Cluster Analysis\\n* System Deployment\\n* Prediction\\n* Deep Learning\\n* Machine Learning\\n* Big Data\\n* Big Data Analytics\\n* Data Mining\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
